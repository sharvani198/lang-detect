{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:** [European Parliament Proceedings Parallel Corpus](http://www.statmt.org/europarl/) is a text dataset used for evaluating language detection engines. The 1.5GB corpus includes 21 languages spoken in EU. Create a machine learning model trained on this dataset to predict the following test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Historically, language classification was done using statistical methods. All languages have certain alphabets or words that could be used to differentiate it from others. But for this method to work we have to maintain dictionaries (or an equivalent) of all languages. This is cumbersome and not scalable to other languages, other dialects or even newer vocabulary. \n",
    "\n",
    "In the last decade or so, people tried to solve this problem using Machine Learning and succeeded! Language Detection is now subsumed in the bigger problem domain of text classification, in which models are trained to assign categories to a given text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem I decided to use Neural Networks, specifically Recurrant Neural Networks. The other contenders outside of NN that could have been used are [N-Grams](http://cloudmark.github.io/Language-Detection-Implemenation/) or [Naive Bayes](https://burakkanber.com/blog/machine-learning-naive-bayes-1/). Within NN the best option is usually with RNN over CNN. Besides, RNNs are what [power Google Translate](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html). Another [Reference](http://cs229.stanford.edu/proj2015/324_report.pdf)\n",
    "\n",
    "Language detection as performed by [Google](https://cloud.google.com/translate/docs/detecting-language) has ~99% accuracy. Undoubtedly the Neural Network architecture they use would be more complex and probably impossible to run on a local machine. Still I'll give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with imports. I decided to use Keras framework running over a tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras import utils\n",
    "from re import sub\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of input text preprocessing needed. For starters we need to remove the html tags in the documents; remove punctuation and numbers (as they don't really help distinguish between European languages);\n",
    "As the test data has sentences as input which we have to label, the training documents are also split into array of sentences.\n",
    "\n",
    "\n",
    "Also it is necessary to convert the string representation into integers. I decided to just use unicode values of each character. \n",
    "\n",
    "Next, it is crucial to make sure that all sentences are equal length (The model requires us to know the input dimensions). If the sentence is >100 characters it is truncated. If it is lesser, the sentence is padded with NULL values in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = sub(\" *<[^>]+> *\",\" \", txt)\n",
    "    txt = sub(\" *\\n *\",\"\\n\",txt)\n",
    "    not_allowed = punctuation + '0123456789'\n",
    "    txt = ''.join([i for i in txt if i not in not_allowed])\n",
    "    \n",
    "    sentences = txt.split(\"\\n\")\n",
    "    sentences = [s for s in sentences if len(s)>1]\n",
    "    return sentences\n",
    "\n",
    "def char_to_int(st):\n",
    "    return [ord(s) for s in st]\n",
    "    \n",
    "def cut_or_pad(st,maxlen):    \n",
    "    if len(st)>=maxlen:\n",
    "        return char_to_int(st[:maxlen])\n",
    "    else:\n",
    "        n_spaces = maxlen - len(st)\n",
    "        return char_to_int(st+'\\x00'*n_spaces )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading of the language files and construction training sets. I put a limit of 200 files due to system constraints. All the text is preprocessed. Sententeces are split and padded/cut (also have a limit). Labels are one hot encoded. Meaning instead of labeling as 'fr', we will label as according to its position in the languages array.( this is just one way to convert the string label into int value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and processing fr\n",
      "Fetching and processing sl\n",
      "Fetching and processing sk\n",
      "Fetching and processing da\n",
      "Fetching and processing es\n",
      "Fetching and processing ro\n",
      "Fetching and processing pl\n",
      "Fetching and processing de\n",
      "Fetching and processing et\n",
      "Fetching and processing sv\n",
      "Fetching and processing fi\n",
      "Fetching and processing lv\n",
      "Fetching and processing el\n",
      "Fetching and processing nl\n",
      "Fetching and processing hu\n",
      "Fetching and processing pt\n",
      "Fetching and processing lt\n",
      "Fetching and processing it\n",
      "Fetching and processing bg\n",
      "Fetching and processing en\n",
      "Fetching and processing cs\n"
     ]
    }
   ],
   "source": [
    "languages = ['fr', 'sl', 'sk', 'da', 'es', 'ro', 'pl', 'de', 'et', 'sv', 'fi', 'lv', 'el', 'nl', 'hu', 'pt', 'lt', 'it', 'bg', 'en', 'cs']\n",
    "train_sentences, train_labels = [],[]\n",
    "num_files = 200\n",
    "maxlen = 100\n",
    "for idx,l in enumerate(languages):\n",
    "    lang_path = \"./txt/\" + l\n",
    "    print(\"Fetching and processing\",l)\n",
    "    all_files = listdir(lang_path)\n",
    "    for f in all_files[:num_files]:\n",
    "        file_path = join(lang_path,f)\n",
    "        with open(file_path, 'r') as txt:\n",
    "            lang_sentences = preprocess(txt.read())\n",
    "            for s in lang_sentences[:100]:\n",
    "                train_labels.append(idx)\n",
    "                train_sentences.append(cut_or_pad(s.lower(), maxlen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same thing is done with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentences, test_labels = [],[]\n",
    "with open(\"./europarl.test\", 'r') as f:\n",
    "    sentences = preprocess(f.read())\n",
    "    for sen in sentences:\n",
    "        s = sen.split(\"\\t\")\n",
    "        if len(s)!=2:\n",
    "            continue\n",
    "        test_labels.append(languages.index(s[0]))\n",
    "        test_sentences.append(cut_or_pad(s[1].lower(), maxlen))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109, 105, 108, 108, 233, 110, 97, 105, 114, 101, 32, 112, 111, 117, 114, 32, 108, 101, 32, 100, 233, 118, 101, 108, 111, 112, 112, 101, 109, 101, 110, 116, 32, 32, 111, 98, 106, 101, 99, 116, 105, 102, 32, 32, 97, 109, 233, 108, 105, 111, 114, 101, 114, 32, 108, 97, 32, 115, 97, 110, 116, 233, 32, 109, 97, 116, 101, 114, 110, 101, 108, 108, 101, 32, 100, 233, 98, 97, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1077, 1074, 1088, 1086, 1087, 1072, 32, 32, 1085, 1077, 32, 1090, 1088, 1103, 1073, 1074, 1072, 32, 1076, 1072, 32, 1089, 1090, 1072, 1088, 1090, 1080, 1088, 1072, 32, 1085, 1086, 1074, 32, 1082, 1086, 1085, 1082, 1091, 1088, 1077, 1085, 1090, 1077, 1085, 32, 1084, 1072, 1088, 1072, 1090, 1086, 1085, 32, 1080, 32, 1080, 1079, 1093, 1086, 1076, 32, 1089, 32, 1087, 1088, 1080, 1074, 1072, 1090, 1080, 1079, 1072, 1094, 1080, 1103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what the max value of the integer encoded sentences are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65633"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value=0\n",
    "for s in train_sentences:\n",
    "    max_value = max(max_value, max(s))\n",
    "\n",
    "for s in test_sentences:\n",
    "    max_value = max(max_value, max(s))\n",
    "\n",
    "max_value = max_value+100\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples  83658\n",
      "Number of test samples  21000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train samples \", len(train_sentences))\n",
    "print(\"Number of test samples \", len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now converting data into numpy arrays. also converting the one-hot encoded labels into a binary matrix so it can be used for multiclass classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = len(languages)\n",
    "x_train = np.asarray(train_sentences)\n",
    "x_test = np.asarray(test_sentences)\n",
    "y_train = utils.to_categorical(train_labels, num_classes)\n",
    "y_test = utils.to_categorical(test_labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be shuffled as all the languages were grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuf = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(shuf)\n",
    "x_train = x_train[shuf]\n",
    "y_train = y_train[shuf]\n",
    "\n",
    "shuf = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(shuf)\n",
    "x_test = x_test[shuf]\n",
    "y_test = y_test[shuf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a portion of the training data as a validation set. This will be used to validate the model and the weights. The model won't be trained on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (73658, 100)\n",
      "x_test shape: (73658, 100)\n",
      "73658 train samples\n",
      "21000 test samples\n",
      "10000 validation samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_valid) = x_train[10000:], x_train[:10000]\n",
    "(y_train, y_valid) = y_train[10000:], y_train[:10000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_valid.shape[0], 'validation samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73658, 100)\n",
      "(73658, 21)\n",
      "(21000, 100)\n",
      "(21000, 21)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture starts with an embedding layer as our integer data will need to become a 3D tensor into order to be consumed by the LSTM cell. LSTM stands for Long Short Term Memory and is a type of RNN layer. It preserves context across the length of the sentence. After LSTM is a Dropout layer to reduce dimensionality and overfittinng. We finish with a fully connected layer that applies a softmax to the model and classfies into one of the languages.  \n",
    "The model is compiled with the loss function and optimizers usually used in multiclass problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 32)           2100256   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 2,185,397.0\n",
      "Trainable params: 2,185,397.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_value, embedding_vector_length, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 10 epochs in batches of 128. We can specify the validation data on which the model will be tested after each epoch. A checkpoint callback is added to make sure that only the weights that performed best against the validation set is stored. This is to make sure that don't use the weights that overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73658 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 1.9927 - acc: 0.2870Epoch 00000: val_loss improved from inf to 1.25246, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 337s - loss: 1.9924 - acc: 0.2869 - val_loss: 1.2525 - val_acc: 0.5092\n",
      "Epoch 2/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 1.0205 - acc: 0.6241Epoch 00001: val_loss improved from 1.25246 to 0.59420, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 380s - loss: 1.0202 - acc: 0.6242 - val_loss: 0.5942 - val_acc: 0.8065\n",
      "Epoch 3/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.5706 - acc: 0.8219Epoch 00002: val_loss improved from 0.59420 to 0.41581, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 390s - loss: 0.5705 - acc: 0.8219 - val_loss: 0.4158 - val_acc: 0.8829\n",
      "Epoch 4/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.4076 - acc: 0.8845Epoch 00003: val_loss improved from 0.41581 to 0.30928, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 409s - loss: 0.4074 - acc: 0.8845 - val_loss: 0.3093 - val_acc: 0.9132\n",
      "Epoch 5/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.3160 - acc: 0.9099Epoch 00004: val_loss improved from 0.30928 to 0.23708, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 411s - loss: 0.3160 - acc: 0.9098 - val_loss: 0.2371 - val_acc: 0.9322\n",
      "Epoch 6/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.2590 - acc: 0.9260Epoch 00005: val_loss did not improve\n",
      "73658/73658 [==============================] - 416s - loss: 0.2590 - acc: 0.9260 - val_loss: 0.2493 - val_acc: 0.9295\n",
      "Epoch 7/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.2233 - acc: 0.9364Epoch 00006: val_loss improved from 0.23708 to 0.19561, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 428s - loss: 0.2232 - acc: 0.9365 - val_loss: 0.1956 - val_acc: 0.9456\n",
      "Epoch 8/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.1961 - acc: 0.9455Epoch 00007: val_loss improved from 0.19561 to 0.16285, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 427s - loss: 0.1960 - acc: 0.9455 - val_loss: 0.1629 - val_acc: 0.9527\n",
      "Epoch 9/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.1682 - acc: 0.9533Epoch 00008: val_loss improved from 0.16285 to 0.14523, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 432s - loss: 0.1682 - acc: 0.9533 - val_loss: 0.1452 - val_acc: 0.9617\n",
      "Epoch 10/10\n",
      "73600/73658 [============================>.] - ETA: 0s - loss: 0.1506 - acc: 0.9594Epoch 00009: val_loss improved from 0.14523 to 0.13647, saving model to model.weights.best.hdf5\n",
      "73658/73658 [==============================] - 435s - loss: 0.1506 - acc: 0.9594 - val_loss: 0.1365 - val_acc: 0.9637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa8ee0a0978>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=(x_valid, y_valid), callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we evaluate the model on test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20992/21000 [============================>.] - ETA: 0s\n",
      "Accuracy: 96.78%\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model.weights.best.hdf5')\n",
    "score = model.evaluate(x_test, y_test)\n",
    "print(\"\\nAccuracy: %.2f%%\" % (score[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view the confusion matrix of the predicted langauage classes. The column values represent the expected labels and the rows are the predicted labels. When both values match, in the diagonal, it means classification was correct.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21000/21000 [==============================] - 41s    \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fr</th>\n",
       "      <th>sl</th>\n",
       "      <th>sk</th>\n",
       "      <th>da</th>\n",
       "      <th>es</th>\n",
       "      <th>ro</th>\n",
       "      <th>pl</th>\n",
       "      <th>de</th>\n",
       "      <th>et</th>\n",
       "      <th>sv</th>\n",
       "      <th>fi</th>\n",
       "      <th>lv</th>\n",
       "      <th>el</th>\n",
       "      <th>nl</th>\n",
       "      <th>hu</th>\n",
       "      <th>pt</th>\n",
       "      <th>lt</th>\n",
       "      <th>it</th>\n",
       "      <th>bg</th>\n",
       "      <th>en</th>\n",
       "      <th>cs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>946</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sl</th>\n",
       "      <td>1</td>\n",
       "      <td>971</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sk</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>965</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>da</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>925</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ro</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pl</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>994</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>de</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>934</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>et</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>939</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sv</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>961</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fi</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>983</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lv</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>995</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>el</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nl</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>962</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hu</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>971</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pt</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>930</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lt</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>985</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>986</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bg</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>980</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cs</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     fr   sl   sk   da   es   ro   pl   de   et   sv   fi   lv    el   nl  \\\n",
       "fr  946    2    0    8    3    1    0    2    3    0    0    1     0    1   \n",
       "sl    1  971    3    3    1    0    2    0    2    0    0    0     0    9   \n",
       "sk    1    8  965    0    1    0    2    0    0    0    1    0     0    0   \n",
       "da    1    0    0  981    0    0    0    0    1    4    0    0     0    9   \n",
       "es    4    2    4    1  925    0    0    0    3    0    0    0     0    0   \n",
       "ro    2    1    0    0    2  980    1    1    4    0    0    0     0    0   \n",
       "pl    0    1    0    1    0    0  994    0    0    0    0    0     0    1   \n",
       "de    0    0    0   11    0    0    1  934    6    7    3    0     0   33   \n",
       "et    4    4    0    4    0    0    0    2  939   11   30    0     0    1   \n",
       "sv    0    0    0   27    0    0    0    2    5  961    4    0     0    0   \n",
       "fi    0    0    0    0    0    0    0    1   10    4  983    0     0    0   \n",
       "lv    0    1    0    0    0    0    0    0    1    0    0  995     0    0   \n",
       "el    0    0    0    0    0    0    0    0    0    0    0    0  1000    0   \n",
       "nl    0    3    0   12    0    0    0    9    0    1    1    0     0  962   \n",
       "hu    0    1   11    0    1    0    6    1    0    4    2    0     0    2   \n",
       "pt    3    0    2    0   50    1    0    0    2    0    0    0     0    1   \n",
       "lt    0    8    0    0    0    0    1    0    4    1    0    0     0    0   \n",
       "it    2    0    0    2    4    0    0    0    0    1    0    0     0    1   \n",
       "bg    0    0    0    0    0    0    0    0    0    0    0    0     0    0   \n",
       "en    1    0    0    8    1    0    0    2    0    0    0    0     0    7   \n",
       "cs    1    0   57    0    1    0    0    0    0    0    0    1     0    1   \n",
       "\n",
       "     hu   pt   lt   it    bg   en   cs  \n",
       "fr    0    2    1    2     0   28    0  \n",
       "sl    0    1    3    3     0    1    0  \n",
       "sk    6    1    5    0     0    0   10  \n",
       "da    0    2    0    0     0    2    0  \n",
       "es    0   39    1   12     0    9    0  \n",
       "ro    0    0    0    4     0    5    0  \n",
       "pl    0    0    1    0     0    2    0  \n",
       "de    0    0    0    0     0    4    1  \n",
       "et    0    2    0    3     0    0    0  \n",
       "sv    0    0    0    0     0    1    0  \n",
       "fi    0    0    2    0     0    0    0  \n",
       "lv    0    0    2    0     0    0    1  \n",
       "el    0    0    0    0     0    0    0  \n",
       "nl    0    0    0    0     0   12    0  \n",
       "hu  971    0    0    0     0    0    1  \n",
       "pt    0  930    0    9     0    2    0  \n",
       "lt    0    0  985    0     0    1    0  \n",
       "it    0    2    0  986     0    2    0  \n",
       "bg    0    0    0    0  1000    0    0  \n",
       "en    0    0    0    1     0  980    0  \n",
       "cs    2    0    1    0     0    0  936  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(x_test, verbose=1)\n",
    "\n",
    "y_pred_labels = np.asarray([ languages[i] for i in np.argmax(y_pred.astype(float), axis=1)])\n",
    "y_test_labels = np.asarray([ languages[i] for i in np.argmax(y_test.astype(float), axis=1)])\n",
    "conf = confusion_matrix(y_test_labels, y_pred_labels, labels=languages)\n",
    "\n",
    "pd.set_option('display.max_columns', 22)\n",
    "pd.DataFrame(conf, index=languages, columns=languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can definately be improved in many ways. \n",
    "1. Use all the language files. I put a limit due to local system's memory constraints.\n",
    "2. Add more LSTM cells/more units in the cells.\n",
    "3. Change the hyperparamenters. Train for longer maybe.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
