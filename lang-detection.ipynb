{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Language Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem Statement:** [European Parliament Proceedings Parallel Corpus](http://www.statmt.org/europarl/) is a text dataset used for evaluating language detection engines. The 1.5GB corpus includes 21 languages spoken in EU. Create a machine learning model trained on this dataset to predict the following test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Historically language classification was done using statistical methods. All language have certain alphabets or words that could be used to differentiate it from others. But for this we had to maintain dictionaries or some equivalent of the languages we would like to detect. This was cumbersome and would also not scale to other languages, other dialects or even newer vocabulary. \n",
    "\n",
    "After that people tried to solve this problem using Machine Learning and succeeded! Language Detection is now subsumed in the bigger problem domain of text classification, which is all about assigning categories to a given text document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this problem I decided to use Neural Networks, specifically Recurrant Neural Networks. The other contenders outside of ML that could have been used are N-Grams or Naive Bayes Classifier. Within ML, the best option is usually with RNN over CNN. Besides, RNNs are what [power Google Translate](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html).\n",
    "\n",
    "Language detection as performed by [Google](https://cloud.google.com/translate/docs/detecting-language) has ~90% accuracy. Undoubtedly the Neural Network architecture they use would be more complex and probably impossible to run on a local machine. Still I'll give it a try."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with imports. I decided to use Keras framework running over a tensorflow backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras import utils\n",
    "from re import sub\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of input text preprocessing needed. For starters we need to remove the html tags in the documents; remove punctuation and numbers (as they don't really help distinguish between European languages);\n",
    "As the test data has sentences as input which we have to label, the training documents are also split into array of sentences.\n",
    "\n",
    "\n",
    "Also necessary it to convert the string representation into integers. I decided to just use unicode values of each character. \n",
    "\n",
    "Next, it is crucial to make sure that all sentences are equal length (The model requires us to know the input dimensions). If the sentence is >200 characters it is truncated. If it is lesser, the sentence is padded with NULL values in the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(txt):\n",
    "    txt = sub(\" *<[^>]+> *\",\" \", txt)\n",
    "    txt = sub(\" *\\n *\",\"\\n\",txt)\n",
    "    not_allowed = punctuation + '0123456789'\n",
    "    txt = ''.join([i for i in txt if i not in not_allowed])\n",
    "    \n",
    "    sentences = txt.split(\"\\n\")\n",
    "    sentences = [s for s in sentences if len(s)>1]\n",
    "    return sentences\n",
    "\n",
    "def char_to_int(st):\n",
    "    return [ord(s) for s in st]\n",
    "    \n",
    "def cut_or_pad(st,maxlen):    \n",
    "    if len(st)>=maxlen:\n",
    "        return char_to_int(st[:maxlen])\n",
    "    else:\n",
    "        n_spaces = maxlen - len(st)\n",
    "        return char_to_int(st+'\\x00'*n_spaces )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading of the language files and construction training sets. I put a limit of 200 files due to system constraints. All the text is preprocessed. Sententeces are split and padded/cut. Labels are one hot encoded. Meaning instead of labeling as 'fr', we will label as according to its position in the languages array.( this is just one way to convert the string label into int value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching and processing fr\n",
      "Fetching and processing sl\n",
      "Fetching and processing sk\n",
      "Fetching and processing da\n",
      "Fetching and processing es\n",
      "Fetching and processing ro\n",
      "Fetching and processing pl\n",
      "Fetching and processing de\n",
      "Fetching and processing et\n",
      "Fetching and processing sv\n",
      "Fetching and processing fi\n",
      "Fetching and processing lv\n",
      "Fetching and processing el\n",
      "Fetching and processing nl\n",
      "Fetching and processing hu\n",
      "Fetching and processing pt\n",
      "Fetching and processing lt\n",
      "Fetching and processing it\n",
      "Fetching and processing bg\n",
      "Fetching and processing en\n",
      "Fetching and processing cs\n"
     ]
    }
   ],
   "source": [
    "languages = ['fr', 'sl', 'sk', 'da', 'es', 'ro', 'pl', 'de', 'et', 'sv', 'fi', 'lv', 'el', 'nl', 'hu', 'pt', 'lt', 'it', 'bg', 'en', 'cs']\n",
    "train_sentences, train_labels = [],[]\n",
    "num_files = 200\n",
    "maxlen = 200\n",
    "for idx,l in enumerate(languages):\n",
    "    lang_path = \"./txt/\" + l\n",
    "    print(\"Fetching and processing\",l)\n",
    "    all_files = listdir(lang_path)\n",
    "    for f in all_files[:num_files]:\n",
    "        file_path = join(lang_path,f)\n",
    "        with open(file_path, 'r') as txt:\n",
    "            lang_sentences = preprocess(txt.read())\n",
    "            for s in lang_sentences:\n",
    "                train_labels.append(idx)\n",
    "                train_sentences.append(cut_or_pad(s.lower(), maxlen))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same this is done with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sentences, test_labels = [],[]\n",
    "with open(\"./europarl.test\", 'r') as f:\n",
    "    sentences = preprocess(f.read())\n",
    "    for sen in sentences:\n",
    "        s = sen.split(\"\\t\")\n",
    "        if len(s)!=2:\n",
    "            continue\n",
    "        test_labels.append(languages.index(s[0]))\n",
    "        test_sentences.append(cut_or_pad(s[1].lower(), maxlen))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[109, 105, 108, 108, 233, 110, 97, 105, 114, 101, 32, 112, 111, 117, 114, 32, 108, 101, 32, 100, 233, 118, 101, 108, 111, 112, 112, 101, 109, 101, 110, 116, 32, 32, 111, 98, 106, 101, 99, 116, 105, 102, 32, 32, 97, 109, 233, 108, 105, 111, 114, 101, 114, 32, 108, 97, 32, 115, 97, 110, 116, 233, 32, 109, 97, 116, 101, 114, 110, 101, 108, 108, 101, 32, 100, 233, 98, 97, 116, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1077, 1074, 1088, 1086, 1087, 1072, 32, 32, 1085, 1077, 32, 1090, 1088, 1103, 1073, 1074, 1072, 32, 1076, 1072, 32, 1089, 1090, 1072, 1088, 1090, 1080, 1088, 1072, 32, 1085, 1086, 1074, 32, 1082, 1086, 1085, 1082, 1091, 1088, 1077, 1085, 1090, 1077, 1085, 32, 1084, 1072, 1088, 1072, 1090, 1086, 1085, 32, 1080, 32, 1080, 1079, 1093, 1086, 1076, 32, 1089, 32, 1087, 1088, 1080, 1074, 1072, 1090, 1080, 1079, 1072, 1094, 1080, 1103, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])\n",
    "print(test_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to know what the max value of the integer encoded sentences are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65633"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_value=0\n",
    "for s in train_sentences:\n",
    "    max_value = max(max_value, max(s))\n",
    "\n",
    "for s in test_sentences:\n",
    "    max_value = max(max_value, max(s))\n",
    "\n",
    "max_value = max_value+100\n",
    "max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples  239930\n",
      "Number of test samples  21000\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of train samples \", len(train_sentences))\n",
    "print(\"Number of test samples \", len(test_sentences))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now converting data into numpy arrays. also converting the one-hot encoded labels into a binary matrix so it can be used for multiclass classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "limit = 70000 # This limit is necessary as numpy arrays of larger size consume more memory. The total array length was >120000\n",
    "num_classes = len(languages)\n",
    "x_train = np.asarray(train_sentences[:limit])\n",
    "x_test = np.asarray(test_sentences)\n",
    "y_train = utils.to_categorical(train_labels[:limit], num_classes)\n",
    "y_test = utils.to_categorical(test_labels, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data needs to be shuffled as all the language were grouped together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuf = np.arange(x_train.shape[0])\n",
    "np.random.shuffle(shuf)\n",
    "x_train = x_train[shuf]\n",
    "y_train = y_train[shuf]\n",
    "\n",
    "\n",
    "shuf = np.arange(x_test.shape[0])\n",
    "np.random.shuffle(shuf)\n",
    "x_test = x_test[shuf]\n",
    "y_test = y_test[shuf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a portion of the training data as a validation set. This will be used to validate the model and the weights. The model won't be trained on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (65000, 200)\n",
      "65000 train samples\n",
      "21000 test samples\n",
      "5000 validation samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_valid) = x_train[5000:], x_train[:5000]\n",
    "(y_train, y_valid) = y_train[5000:], y_train[:5000]\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print(x_valid.shape[0], 'validation samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65000, 200)\n",
      "(65000, 21)\n",
      "(21000, 200)\n",
      "(21000, 21)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model architecture starts with an embedding layer as our integer data will need to become a 3D tensor into order to be consumed by the LSTM cell. LSTM stands for Long Short Term Memory and is a type of RNN layer. It preserves context across the length of the sentence. After LSTM is a Dropout layer to reduce dimensionality and overfittinng. We finish with a fully connected layer that applies a softmax to the model and classfies into one of the languages.  \n",
    "The model is compiled with the loss function and optimizers usually used in multiclass problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 200, 32)           2100256   \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 128)               82432     \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21)                2709      \n",
      "=================================================================\n",
      "Total params: 2,185,397.0\n",
      "Trainable params: 2,185,397.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_value, embedding_vector_length, input_length=maxlen))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 10 epochs in batches of 128. We can specify the validation data on which the model will be tested after each epoch. A checkpoint callback is added to make sure that only the weights that performed best against the validation set is stored. This is to make sure that don't use the weights that overfit to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65000 samples, validate on 5000 samples\n",
      "Epoch 1/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.7470 - acc: 0.7145Epoch 00000: val_loss improved from inf to 0.80731, saving model to model.weights.best.hdf5\n",
      "65000/65000 [==============================] - 519s - loss: 0.7469 - acc: 0.7146 - val_loss: 0.8073 - val_acc: 0.7526\n",
      "Epoch 2/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.3999 - acc: 0.8931Epoch 00001: val_loss improved from 0.80731 to 0.13264, saving model to model.weights.best.hdf5\n",
      "65000/65000 [==============================] - 497s - loss: 0.3995 - acc: 0.8932 - val_loss: 0.1326 - val_acc: 0.9636\n",
      "Epoch 3/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.2803 - acc: 0.9284Epoch 00002: val_loss improved from 0.13264 to 0.11620, saving model to model.weights.best.hdf5\n",
      "65000/65000 [==============================] - 489s - loss: 0.2799 - acc: 0.9285 - val_loss: 0.1162 - val_acc: 0.9654\n",
      "Epoch 4/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.1738 - acc: 0.9556Epoch 00003: val_loss did not improve\n",
      "65000/65000 [==============================] - 506s - loss: 0.1739 - acc: 0.9556 - val_loss: 0.1445 - val_acc: 0.9620\n",
      "Epoch 5/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.1245 - acc: 0.9710Epoch 00004: val_loss improved from 0.11620 to 0.10261, saving model to model.weights.best.hdf5\n",
      "65000/65000 [==============================] - 522s - loss: 0.1244 - acc: 0.9710 - val_loss: 0.1026 - val_acc: 0.9730\n",
      "Epoch 6/8\n",
      "64896/65000 [============================>.] - ETA: 0s - loss: 0.1052 - acc: 0.9718Epoch 00005: val_loss did not improve\n",
      "65000/65000 [==============================] - 562s - loss: 0.1055 - acc: 0.9718 - val_loss: 0.1329 - val_acc: 0.9574\n",
      "Epoch 7/8\n",
      " 9600/65000 [===>..........................] - ETA: 483s - loss: 0.0871 - acc: 0.9769"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, \n",
    "                               save_best_only=True)\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=8, validation_data=(x_valid, y_valid), callbacks=[checkpointer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we evaluate the model on test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.weights.best.hdf5')\n",
    "scores = model.evaluate(x_test, y_test)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can definately be improved in many ways. \n",
    "1. Use all the language files. I put a limit due to local system's memory constraints.\n",
    "2. Add more LSTM cells/more units in the cells.\n",
    "3. Change the hyperparamenters. Train for longer maybe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
